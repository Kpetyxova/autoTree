# A Fully Automated Pipeline for Conversational Discourse Annotation  
### Tree Scheme Generation and Labeling with Large Language Models

Recent advances in Large Language Models (LLMs) have shown great potential in automating discourse annotation for conversations. While manually designing tree-based annotation schemes improves annotation quality for both humans and models, the process remains time-consuming and requires expert knowledge.

We propose a **fully automated pipeline** that uses LLMs to construct these annotation schemes and perform labeling. We evaluate our approach on **Speech Functions (SFs)** and the **Switchboard-DAMSL (SWBD-DAMSL)** taxonomies. Our experiments compare different design choices and demonstrate that **frequency-guided decision trees**, combined with advanced LLMs, can **outperform manually designed trees** and even **match or surpass human annotators**‚Äîwhile significantly reducing annotation time.

We release all code, annotation schemes, and outputs to support further research in conversational discourse analysis.

![Pipeline for tree construction. LLM formulates a classification question to split classes into groups, mapping possible answers to respective class groups. This process repeats recursively for created groups until all groups contain only one class. Finally, the grouped data is merged into a single tree structure in JSON format for annotation.](autotree_pipeline.png)

---

## üìÅ Project Structure

```
autoTree/
‚îú‚îÄ‚îÄ annotate.py           # Annotation script
‚îú‚îÄ‚îÄ create_tree.py        # Tree generation logic
‚îú‚îÄ‚îÄ eval.py               # General evaluation script
‚îú‚îÄ‚îÄ utils.py              # Utility functions
‚îú‚îÄ‚îÄ requirements.txt      # Project dependencies
‚îú‚îÄ‚îÄ prompts/              # Prompt templates
‚îú‚îÄ‚îÄ sf/                   # Speech Functions components
‚îú‚îÄ‚îÄ mrbench/              # MRBench-specific files
‚îú‚îÄ‚îÄ mathdial/             # MathDIAL-specific files
‚îî‚îÄ‚îÄ swda/                 # Switchboard-DAMSL corpus files
```


## ‚öôÔ∏è Installation
```bash
pip install -r requirements.txt
```

## üå≥ Tree Creation

### Required Arguments:
- `-t, --taxonomy_path`: Path to your taxonomy file (CSV, JSON, or TSV format)
- `-d, --description_path`: Path to the description file (for label descriptions)
- `-o, --output_path`: Directory to save the output files

### Optional Argument:
- `-c, --config`: Configuration type for tree creation  
  **Default:** `"yes_no"`  
  **Options:**
  - `yes_no`
  - `free_form_binary`
  - `free_form_non_binary`
  - `split_selection`
  - `freq_guided_split_selection`
  - `freq_guided_free_form_non_binary`
  - `yes_no_mrbench`

### Example:

```bash
python create_tree.py -t sf/two_levels/speech_functions_two_levels.tsv -d sf/description.txt -o sf/two_levels/taxonomy_tree.json -c free_form_non_binary
```

### This will:
1. Load the taxonomy from `speech_functions_two_levels.tsv` (must include a `Labels` column)
2. Load label descriptions from `description.txt`
3. Generate a decision tree and save it to `taxonomy_tree.json`
4. Also produce:
   - `taxonomy_tree.png` ‚Äî Tree visualization
   - `taxonomy_tree.txt` ‚Äî Text-based tree representation

---

## üìù Annotation

### Required Arguments:
- `-d, --dialogs_path`: Path to the dialog file (`.csv`, `.json`, or `.tsv`). Must contain columns: `dialog_id`, `speaker`, `text`.
- `-t, --tree_path`: Path to the decision tree `.json` file
- `-o, --output_dir`: Directory to save the output annotations

### Optional Arguments:
- `-m, --model`: OpenAI model to use (default: `"gpt-4o"`)
- `-b, --binary`: Use `True` if using a binary tree (e.g., with `yes_no`, `free_form_binary`, or `yes_no_mrbench` configs); default is `False`

### Example:

```bash
python annotate.py -d sf/gold_standard_test.csv -t sf/two_levels/taxonomy_tree.json -o sf/two_levels -m gpt-4o
```

### This will:
1. Load the dialog dataset
2. Annotate each utterance using the LLM and decision tree
3. Save the results to `dialogs_annotated.tsv` in the output directory

### ‚ö†Ô∏è Notes:
- Ensure your `.env` file contains a valid `OPENAI_API_KEY`
- The dialog file must include `dialog_id`, `speaker`, and `text`
- The tree must be a `.json` file in the format generated by `create_tree.py`

---

## üìä Evaluation

If you have gold annotations and want to compare them with your predicted labels, use `eval.py`.

### Required Arguments:
- `-d, --dialogs_path`: Path to the gold-standard dialog file (must have column `Annotations_gold`)
- `-a, --annotations_path`: Path to predicted annotations file (must have column `Annotations`)
- `-o, --output_dir`: Directory to save evaluation results

### Output:
- A file `eval.txt` containing:
  - Precision
  - Recall
  - F1-score
  - Support per class

### Example:
```bash
python eval.py -d /path/to/gold_dialogs.tsv -a /path/to/predicted_annotations.tsv -o /path/to/evaluation/results
```

## üóÇ SF-Specific Evaluation

If you‚Äôre evaluating Speech Functions, use the dedicated script:
```bash
python sf/eval_sf.py -g sf/all_gold_train.json -a sf/two_levels/dialogs_annotated.tsv -o sf/two_levels -d sf/gold_standard_test.csv -l second
```